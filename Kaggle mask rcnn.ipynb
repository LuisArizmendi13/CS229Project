{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3de4394b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1450/1450 [00:02<00:00, 599.24it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 307/307 [00:00<00:00, 908.07it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 743/743 [00:00<00:00, 975.08it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from PIL import Image\n",
    "import glob\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "import ast\n",
    "import numpy as np\n",
    "from PIL import ImageDraw\n",
    "from torchvision.transforms import Compose, Resize, Pad, ToTensor\n",
    "from PIL import ImageOps\n",
    "\n",
    "\n",
    "class StrawberryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_list, data_labels, transforms=None):\n",
    "        self.image_list = image_list\n",
    "        self.data_labels = data_labels\n",
    "        self.transforms = transforms\n",
    "        self.label_to_id = {\n",
    "            'Angular Leafspot': 0,\n",
    "            'Anthracnose Fruit Rot': 1,\n",
    "            'Blossom Blight': 2,\n",
    "            'Gray Mold': 3,\n",
    "            'Leaf Spot': 4,\n",
    "            'Powdery Mildew Fruit': 5,\n",
    "            'Powdery Mildew Leaf': 6\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.image_list[idx]\n",
    "        img_path = self.data_labels.iloc[idx]['image']\n",
    "        labels = self.data_labels.iloc[idx]['label']\n",
    "        label_ids = [self.label_to_id[label] for label in labels]\n",
    "        points_list = self.data_labels.iloc[idx]['points']\n",
    "\n",
    "        width, height = img.size\n",
    "        masks = []\n",
    "        bboxes = []\n",
    "        areas = []\n",
    "\n",
    "        for points in points_list:\n",
    "            points_tuples = [tuple(point) for point in points]\n",
    "            # Create the mask\n",
    "            mask = Image.new('L', (width, height))\n",
    "            draw = ImageDraw.Draw(mask)\n",
    "            draw.polygon(points_tuples, fill=1)\n",
    "\n",
    "            # Calculate the bounding box\n",
    "            xy = [point for sublist in points for point in sublist]\n",
    "            x_values = xy[0::2]\n",
    "            y_values = xy[1::2]\n",
    "            x_min, x_max = min(x_values), max(x_values)\n",
    "            y_min, y_max = min(y_values), max(y_values)\n",
    "            bbox = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "            mask = torch.tensor(np.array(mask), dtype=torch.uint8)\n",
    "            masks.append(mask)\n",
    "            bboxes.append(bbox)\n",
    "            areas.append((x_max - x_min) * (y_max - y_min))\n",
    "\n",
    "        # Stack masks along the first dimension\n",
    "        masks = torch.stack(masks, dim=0)\n",
    "\n",
    "        target = {\n",
    "            'boxes': torch.tensor(bboxes, dtype=torch.float32),\n",
    "            'labels': torch.tensor(label_ids, dtype=torch.int64),\n",
    "            'masks': masks,\n",
    "            'image_id': torch.tensor([idx]),\n",
    "            'area': torch.tensor(areas, dtype=torch.float32),\n",
    "            'iscrowd': torch.tensor([0] * len(labels), dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def get_transform(img, target, min_dim=512, max_dim=960):\n",
    "    def resize_and_pad_fn(img, target):\n",
    "        w, h = img.size\n",
    "        scale_factor = min(min_dim / min(w, h), max_dim / max(w, h))\n",
    "        new_w = int(w * scale_factor)\n",
    "        new_h = int(h * scale_factor)\n",
    "\n",
    "        img = img.resize((new_w, new_h), Image.BILINEAR)\n",
    "        padding = (min_dim - new_w) // 2, (min_dim - new_h) // 2\n",
    "        img = ImageOps.expand(img, padding)\n",
    "\n",
    "        # Apply the same padding to the target's bounding boxes\n",
    "        target['boxes'][:, :2] += torch.tensor(padding, dtype=torch.float32)\n",
    "        target['boxes'][:, 2:] += torch.tensor(padding, dtype=torch.float32)\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "    img, target = resize_and_pad_fn(img, target) \n",
    "    img = to_tensor(img)\n",
    "    return img, target\n",
    "\n",
    "# Extract train, test, val images and labels\n",
    "\n",
    "def load_data(data_dir):\n",
    "    images = [] \n",
    "    labels = {} # to capture image name as key and corresponding label out of json as value\n",
    "    points = {}\n",
    "    \n",
    "    elems = glob.glob(os.path.join(data_dir, '*.jpg'))\n",
    "    elems = sorted(elems)\n",
    "    \n",
    "    i = 0\n",
    "    for elem in tqdm.tqdm(elems):\n",
    "        # Read image\n",
    "        img = Image.open(elem)\n",
    "        images.append(img)\n",
    "    \n",
    "        # Read label path\n",
    "        label_path = elem.lower().replace('jpg','json')\n",
    "    \n",
    "        # Read labels from json file\n",
    "        f = open(label_path)\n",
    "        label_data = json.load(f)\n",
    "        \n",
    "        image_labels = []\n",
    "        image_points = []\n",
    "        for shapes in label_data['shapes']:\n",
    "            label = shapes['label']\n",
    "            point = shapes['points']\n",
    "            image_labels.append(label)\n",
    "            image_points.append(point)\n",
    "\n",
    "        labels[label_data['imagePath']] = image_labels\n",
    "        points[label_data['imagePath']] = image_points\n",
    "        \n",
    "        #for testing with smaller data volume\n",
    "        #if i == 10:\n",
    "            #break\n",
    "        i+=1\n",
    "        \n",
    "    df = pd.DataFrame(list(labels.items()), columns=[\"image\", \"label\"])\n",
    "    df_points = pd.DataFrame(list(points.items()), columns=[\"image\", \"points\"])\n",
    "    df = df.merge(df_points, on=\"image\")\n",
    "        \n",
    "    return images, df\n",
    "\n",
    "train_dir = \"archive (1)/train/\"\n",
    "val_dir = \"archive (1)/val/\"\n",
    "test_dir = \"archive (1)/test/\"\n",
    "\n",
    "train_images, df_train_labels = load_data(train_dir)\n",
    "test_images, df_test_labels = load_data(val_dir)\n",
    "val_images, df_val_labels = load_data(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dda5dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from pprint import pprint\n",
    "\n",
    "def simple_mean_average_precision(targets, preds):\n",
    "    # Convert input format to the format required by the metric\n",
    "    formatted_targets = []\n",
    "    formatted_preds = []\n",
    "    \n",
    "    for target, pred in zip(targets, preds):\n",
    "        formatted_targets.append({\n",
    "            'boxes': target['boxes'],\n",
    "            'labels': target['labels'],\n",
    "        })\n",
    "        formatted_preds.append({\n",
    "            'boxes': pred['boxes'],\n",
    "            'scores': pred['scores'],\n",
    "            'labels': pred['labels'],\n",
    "        })\n",
    "\n",
    "    #print(\"\\n\\nFormatted Targets: \", formatted_targets[0])\n",
    "    #print(\"\\n\\nFormatted Preds: \", formatted_preds[0])\n",
    "    # Initialize the MeanAveragePrecision metric\n",
    "    metric = MeanAveragePrecision()\n",
    "\n",
    "    # Update and compute the metric\n",
    "    metric.update(formatted_preds, formatted_targets)\n",
    "    result = metric.compute()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "530bd4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/larizmen/micromamba/envs/strawberry_detection/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/larizmen/micromamba/envs/strawberry_detection/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/tmp/ipykernel_20900/1834025671.py:49: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/tmp/ipykernel_20900/1834025671.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training epoch 1/50: 100%|████████████████████████████████████████████████████████████| 725/725 [03:40<00:00,  3.29it/s]\n",
      "/tmp/ipykernel_20900/1834025671.py:101: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validating epoch 1/50:   0%|                                                                    | 0/154 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "`MAP` metric requires that `pycocotools` or `faster-coco-eval` installed. Please install with `pip install pycocotools` or `pip install faster-coco-eval` or `pip install torchmetrics[detection]`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 103\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[1;32m    102\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model(images)  \u001b[38;5;66;03m# Get predictions\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m mAP_dict \u001b[38;5;241m=\u001b[39m \u001b[43msimple_mean_average_precision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Update the running sum of each mAP metric\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m mAP_dict\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[8], line 23\u001b[0m, in \u001b[0;36msimple_mean_average_precision\u001b[0;34m(targets, preds)\u001b[0m\n\u001b[1;32m     14\u001b[0m     formatted_preds\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m'\u001b[39m: pred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m: pred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: pred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     18\u001b[0m     })\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#print(\"\\n\\nFormatted Targets: \", formatted_targets[0])\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#print(\"\\n\\nFormatted Preds: \", formatted_preds[0])\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Initialize the MeanAveragePrecision metric\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m metric \u001b[38;5;241m=\u001b[39m \u001b[43mMeanAveragePrecision\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Update and compute the metric\u001b[39;00m\n\u001b[1;32m     26\u001b[0m metric\u001b[38;5;241m.\u001b[39mupdate(formatted_preds, formatted_targets)\n",
      "File \u001b[0;32m~/micromamba/envs/strawberry_detection/lib/python3.9/site-packages/torchmetrics/detection/mean_ap.py:389\u001b[0m, in \u001b[0;36mMeanAveragePrecision.__init__\u001b[0;34m(self, box_format, iou_type, iou_thresholds, rec_thresholds, max_detection_thresholds, class_metrics, extended_summary, average, backend, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (_PYCOCOTOOLS_AVAILABLE \u001b[38;5;129;01mor\u001b[39;00m _FASTER_COCO_EVAL_AVAILABLE):\n\u001b[0;32m--> 389\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[1;32m    390\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`MAP` metric requires that `pycocotools` or `faster-coco-eval` installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please install with `pip install pycocotools` or `pip install faster-coco-eval` or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `pip install torchmetrics[detection]`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    393\u001b[0m     )\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _TORCHVISION_AVAILABLE:\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[1;32m    396\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetric `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iou_type\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` requires that `torchvision` is installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please install with `pip install torchmetrics[detection]`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    398\u001b[0m     )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: `MAP` metric requires that `pycocotools` or `faster-coco-eval` installed. Please install with `pip install pycocotools` or `pip install faster-coco-eval` or `pip install torchmetrics[detection]`."
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torchvision.models.detection import MaskRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "def get_model(type, num_classes = 7):\n",
    "    if type == \"baseline\":\n",
    "        model = maskrcnn_resnet50_fpn(pretrained=True)\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "        in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "        hidden_layer = 256\n",
    "        model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(\n",
    "            in_features_mask, hidden_layer, num_classes)\n",
    "        model.to(device)\n",
    "        return model\n",
    "    \n",
    "def get_optimizer(type, batch_size, gradient_accumulation_steps, lr=0.0001, momentum=0.9, weight_decay=0.0001):\n",
    "    if type == \"SGD\":\n",
    "        return torch.optim.SGD(params, lr=lr*batch_size*gradient_accumulation_steps/2, momentum=momentum, weight_decay=weight_decay)\n",
    "        \n",
    "batch_size = 2\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_data = StrawberryDataset(train_images, df_train_labels, transforms=get_transform)\n",
    "val_data = StrawberryDataset(test_images, df_test_labels, transforms=get_transform)\n",
    "test_data = StrawberryDataset(val_images, df_val_labels, transforms=get_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_classes = 7  # Your dataset has 7 classes (labels range from 0 to 6)\n",
    "model = get_model(\"baseline\", num_classes)\n",
    "\n",
    "# Set up the optimizer and learning rate scheduler\n",
    "gradient_accumulation_steps = 4\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = get_optimizer(\"SGD\", batch_size=batch_size, gradient_accumulation_steps=gradient_accumulation_steps, lr=0.0001)\n",
    "#lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "scaler = GradScaler()\n",
    "num_epochs = 50\n",
    "history = pd.DataFrame(columns=[\"Epoch\", \"Train Loss\", \"Val Loss\", \"Train mAP\", \"Val mAP\"])\n",
    "for epoch in range(num_epochs):\n",
    "    # Train the model for one epoch\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    train_mAP = 0\n",
    "    total_train_samples = 0\n",
    "    step = 0\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    for images, targets in tqdm(train_loader, desc=f\"Training epoch {epoch + 1}/{num_epochs}\"):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        with autocast():\n",
    "            loss_dict = model(images, targets)\n",
    "            #predictions = model(images)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        scaler.scale(losses / gradient_accumulation_steps).backward()\n",
    "        running_loss += losses.cpu().detach().item()\n",
    "\n",
    "        # Calculate train mAP\n",
    "        #train_mAP += simple_mean_average_precision(targets, predictions, num_classes=num_classes)\n",
    "        total_train_samples += len(targets)\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        step += 1\n",
    "\n",
    "    # Use learning rate scheduler\n",
    "    # r_scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_mAP = []\n",
    "    total_val_samples = 0\n",
    "    predictions = []\n",
    "\n",
    "    # Initialize a dictionary to store the running sum of each mAP metric\n",
    "    mAP_sums = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(val_loader, desc=f\"Validating epoch {epoch + 1}/{num_epochs}\"):\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            with autocast():\n",
    "                predictions = model(images)  # Get predictions\n",
    "            mAP_dict = simple_mean_average_precision(targets, predictions)\n",
    "\n",
    "            # Update the running sum of each mAP metric\n",
    "            for key, value in mAP_dict.items():\n",
    "                if key in mAP_sums:\n",
    "                    mAP_sums[key] += value.cpu().detach().numpy()\n",
    "                else:\n",
    "                    mAP_sums[key] = value.numpy()\n",
    "\n",
    "            #losses = sum(loss for loss in loss_dict.values())\n",
    "            #val_loss += losses.item()\n",
    "\n",
    "            total_val_samples += len(targets)\n",
    "\n",
    "    # Calculate the average of each mAP metric\n",
    "    mAP_averages = {key: value / total_val_samples for key, value in mAP_sums.items()}\n",
    "    val_mAP.append(mAP_averages)\n",
    "\n",
    "    train_loss = running_loss / (step + 1)\n",
    "    #print(f\"Epoch: {epoch + 1}, Train Loss: {train_loss}, Val mAP: {val_mAP[-1]}\")\n",
    "    print(f\"Epoch: {epoch + 1}, Train Loss: {train_loss}, Val mAP: {mAP_averages}\")\n",
    "    # Append the current epoch results to the history DataFrame\n",
    "    history = history.append({\n",
    "        \"Epoch\": epoch + 1,\n",
    "        \"Train Loss\": train_loss,\n",
    "        #\"Val Loss\": val_loss,\n",
    "        #\"Train mAP\": train_mAP,\n",
    "        \"Val mAP\": val_mAP},\n",
    "        ignore_index=True)\n",
    "\n",
    "history.to_csv(\"training_history.csv\", index=False)\n",
    "torch.save(model.state_dict(), \"mask_rcnn_strawberry.pth\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "test_mAP = []\n",
    "total_test_samples = 0\n",
    "predictions_test = []\n",
    "# Initialize a dictionary to store the running sum of each mAP metric\n",
    "mAP_test_sums = {}\n",
    "with torch.no_grad():\n",
    "    for images, targets in test_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with autocast():\n",
    "            predictions += model(images)  # Get predictions\n",
    "            mAP_dict = simple_mean_average_precision(targets, predictions)\n",
    "\n",
    "            # Update the running sum of each mAP metric\n",
    "            for key, value in mAP_dict.items():\n",
    "                if key in mAP_test_sums:\n",
    "                    mAP_test_sums[key] += value.cpu().detach().numpy()\n",
    "                else:\n",
    "                    mAP_test_sums[key] = value.cpu().detach().numpy()\n",
    "\n",
    "            #losses = sum(loss for loss in loss_dict.values())\n",
    "            #val_loss += losses.item()\n",
    "\n",
    "            total_test_samples += len(targets)\n",
    "\n",
    "    # Calculate the average of each mAP metric\n",
    "    mAP_test_averages = {key: value / total_val_samples for key, value in mAP_test_sums.items()}\n",
    "    print(f\"Test mAP: {mAP_test_averages}\")\n",
    "\n",
    "print(\"Finished evaluating the model on the test set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e23a927-729f-4200-883e-bbcc488abf76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (strawberry_detection)",
   "language": "python",
   "name": "strawberry_detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
